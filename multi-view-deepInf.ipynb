{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepInf.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2fuw8wGuFuV8Hnu58GNUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahmatjafari/infPredict/blob/main/multi-view-deepInf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsC2NW_M0kwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f56ab0-0c6c-4c97-d2a0-b76c893521e5"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install tensorboard_logger\n",
        "!pip install python-igraph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboard_logger\n",
            "  Downloading tensorboard_logger-0.1.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (3.17.3)\n",
            "Installing collected packages: tensorboard-logger\n",
            "Successfully installed tensorboard-logger-0.1.0\n",
            "Collecting python-igraph\n",
            "  Downloading python_igraph-0.9.6-cp37-cp37m-manylinux2010_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, python-igraph\n",
            "Successfully installed python-igraph-0.9.6 texttable-1.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PygCEVtu6mXX"
      },
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3QAh1Q86n-p"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqsqFx7n6rnY"
      },
      "source": [
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3mJlwrv671q"
      },
      "source": [
        "file_list = drive.ListFile(\n",
        "    {'q': \"'10PtS86ngpUYOcMaNnNF8_-eBi5wUcXWQ' in parents\"}).GetList()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukTOf4Cx7cpU",
        "outputId": "d093efda-5a58-41d0-d5b0-f1ef9db32ac2"
      },
      "source": [
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title: vertex_id.npy, id: 1JmZhCaiyAocjt_Tv5taOeHm2rE6rMTw2\n",
            "downloading to /root/data/vertex_id.npy\n",
            "title: label.npy, id: 1zagC1M8qHUC92EK-ZuBjCeXIxnM4Lta8\n",
            "downloading to /root/data/label.npy\n",
            "title: adjacency_matrix.npy, id: 1FHTCb8ZwbrRBbXtMPUf2sRYtuQOmsaM8\n",
            "downloading to /root/data/adjacency_matrix.npy\n",
            "title: influence_feature.npy, id: 1-FAz3vustocAKoMLvij1vNhzrIjfFtLV\n",
            "downloading to /root/data/influence_feature.npy\n",
            "title: deepwalk.emb_64, id: 1a3vkor8iOnFBPeABY-kqpZ6kNLBRdd-j\n",
            "downloading to /root/data/deepwalk.emb_64\n",
            "title: vertex_feature.npy, id: 1JsM7qaLIeoAIM9MUlJzeYN9UEXOVARgb\n",
            "downloading to /root/data/vertex_feature.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8igOpcy9Z18"
      },
      "source": [
        "labels_path = os.path.join(local_download_path, 'label.npy')\n",
        "adjacency_matrix_path = os.path.join(local_download_path, 'adjacency_matrix.npy')\n",
        "deepwalk_path = os.path.join(local_download_path, 'deepwalk.emb_64')\n",
        "influence_feature_path = os.path.join(local_download_path, 'influence_feature.npy')\n",
        "vertex_feature_path = os.path.join(local_download_path, 'vertex_feature.npy')\n",
        "vertex_id_path = os.path.join(local_download_path, 'vertex_id.npy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHVD2GAZ_JFf"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import Sampler\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import sklearn\n",
        "import itertools\n",
        "import logging\n",
        "import igraph\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import time\n",
        "import argparse\n",
        "import shutil\n",
        "import logging\n",
        "from tensorboard_logger import tensorboard_logger\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ms6S-GoAA0E"
      },
      "source": [
        "def load_w2v_feature(file, max_idx=0):\n",
        "    with open(file, \"rb\") as f:\n",
        "        nu = 0\n",
        "        for line in f:\n",
        "            content = line.strip().split()\n",
        "            nu += 1\n",
        "            if nu == 1:\n",
        "                n, d = int(content[0]), int(content[1])\n",
        "                feature = [[0.] * d for i in range(max(n, max_idx + 1))]\n",
        "                continue\n",
        "            index = int(content[0])\n",
        "            while len(feature) <= index:\n",
        "                feature.append([0.] * d)\n",
        "            for i, x in enumerate(content[1:]):\n",
        "                feature[index][i] = float(x)\n",
        "    for item in feature:\n",
        "        assert len(item) == d\n",
        "    return np.array(feature, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK4pxDhVAeh5"
      },
      "source": [
        "class ChunkSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Samples elements sequentially from some offset.\n",
        "    Arguments:\n",
        "        num_samples: # of desired datapoints\n",
        "        start: offset where we should start selecting from\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples, start=0):\n",
        "        self.num_samples = num_samples\n",
        "        self.start = start\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(range(self.start, self.start + self.num_samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "class InfluenceDataSet(Dataset):\n",
        "    def __init__(self, file_dir, embedding_dim, seed, shuffle, model):\n",
        "        self.graphs = np.load(os.path.join(file_dir, \"adjacency_matrix.npy\")).astype(np.float32)\n",
        "\n",
        "        # self-loop trick, the input graphs should have no self-loop\n",
        "        identity = np.identity(self.graphs.shape[1])\n",
        "        self.graphs += identity\n",
        "        self.graphs[self.graphs != 0] = 1.0\n",
        "\n",
        "        # normalized graph laplacian for GCN: D^{-1/2}AD^{-1/2}\n",
        "        for i in range(len(self.graphs)):\n",
        "            graph = self.graphs[i]\n",
        "            d_root_inv = 1. / np.sqrt(np.sum(graph, axis=1))\n",
        "            graph = (graph.T * d_root_inv).T * d_root_inv\n",
        "\n",
        "        logger.info(\"graphs loaded!\")\n",
        "\n",
        "        # wheather a user has been influenced\n",
        "        # wheather he/she is the ego user\n",
        "        self.influence_features = np.load(\n",
        "                os.path.join(file_dir, \"influence_feature.npy\")).astype(np.float32)\n",
        "        logger.info(\"influence features loaded!\")\n",
        "\n",
        "        self.labels = np.load(os.path.join(file_dir, \"label.npy\"))\n",
        "        logger.info(\"labels loaded!\")\n",
        "\n",
        "        self.vertices = np.load(os.path.join(file_dir, \"vertex_id.npy\"))\n",
        "        logger.info(\"vertex ids loaded!\")\n",
        "\n",
        "        if shuffle:\n",
        "            self.graphs, self.influence_features, self.labels, self.vertices = \\\n",
        "                    sklearn.utils.shuffle(\n",
        "                        self.graphs, self.influence_features,\n",
        "                        self.labels, self.vertices,\n",
        "                        random_state=seed\n",
        "                    )\n",
        "\n",
        "        vertex_features = np.load(os.path.join(file_dir, \"vertex_feature.npy\"))\n",
        "        vertex_features = preprocessing.scale(vertex_features)\n",
        "        self.vertex_features = torch.FloatTensor(vertex_features)\n",
        "        logger.info(\"global vertex features loaded!\")\n",
        "\n",
        "        embedding_path = os.path.join(file_dir, \"deepwalk.emb_%d\" % embedding_dim)\n",
        "        max_vertex_idx = np.max(self.vertices)\n",
        "        embedding = load_w2v_feature(embedding_path, max_vertex_idx)\n",
        "        self.embedding = torch.FloatTensor(embedding)\n",
        "        logger.info(\"%d-dim embedding loaded!\", embedding_dim)\n",
        "\n",
        "        self.N = self.graphs.shape[0]\n",
        "        logger.info(\"%d ego networks loaded, each with size %d\" % (self.N, self.graphs.shape[1]))\n",
        "\n",
        "        n_classes = self.get_num_class()\n",
        "        class_weight = self.N / (n_classes * np.bincount(self.labels))\n",
        "        self.class_weight = torch.FloatTensor(class_weight)\n",
        "\n",
        "    def get_embedding(self):\n",
        "        return self.embedding\n",
        "\n",
        "    def get_influence_features(self):\n",
        "        return self.influence_features\n",
        "\n",
        "    def get_vertex_features(self):\n",
        "        return self.vertex_features\n",
        "\n",
        "    def get_feature_dimension(self):\n",
        "        return self.influence_features.shape[-1]\n",
        "\n",
        "    def get_num_class(self):\n",
        "        return np.unique(self.labels).shape[0]\n",
        "\n",
        "    def get_class_weight(self):\n",
        "        return self.class_weight\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx], self.influence_features[idx], self.labels[idx], self.vertices[idx]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU7HHkc_PM9l"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s') # include timestamp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vbcTILVHE2G"
      },
      "source": [
        "# Training settings\n",
        "class args:\n",
        "  tensorboard_log=''              #name of this run\n",
        "  model = 'gcn'                   #models used\n",
        "  no_cuda=False                   #Disables CUDA training.\n",
        "  seed=42                         #Random seed.\n",
        "  epochs=1000                     #Number of epochs to train.\n",
        "  lr=0.05                         #Initial learning rate.\n",
        "  weight_decay=1e-3               #Weight decay (L2 loss on parameters).\n",
        "  dropout=0.2                     #Dropout rate (1 - keep probability).\n",
        "  hidden_units=\"16,8\"             #Hidden units in each hidden layer, splitted with comma\n",
        "  heads=\"1,1,1\"                   #Heads in each layer, splitted with comma\n",
        "  batch=128                      #Batch size\n",
        "  dim=64                          #Embedding dimension\n",
        "  check_point=10                  #Eheck point\n",
        "  instance_normalization=True    #Enable instance normalization\n",
        "  shuffle=False                   #Shuffle dataset\n",
        "  file_dir=local_download_path    #Input file directory\n",
        "  train_ratio=75                  #Training ratio (0, 100)\n",
        "  valid_ratio=12.5                  #Validation ratio (0, 100)\n",
        "  class_weight_balanced=False     #Adjust weights inversely proportional to class frequencies in the input data\n",
        "  use_vertex_feature=True         #Whether to use vertices' structural features\n",
        "  sequence_size=16                #Sequence size (only useful for pscn)\n",
        "  neighbor_size=5                 #Neighborhood size (only useful for pscn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_gfpfELLJ8s"
      },
      "source": [
        "args.cuda = not args.no_cuda and torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCQWIEXgOe0x"
      },
      "source": [
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSNy6Kl8O6DP"
      },
      "source": [
        "# adj N*n*n\n",
        "# feature N*n*f\n",
        "# labels N*n*c\n",
        "# Load data\n",
        "# vertex: vertex id in global network N*n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b986Tif8Qa8t",
        "outputId": "e4ac3c2e-c609-48ff-ae25-3eae5e670b8f"
      },
      "source": [
        "influence_dataset = InfluenceDataSet(\n",
        "            args.file_dir, args.dim, args.seed, args.shuffle, args.model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 17:48:46,681 graphs loaded!\n",
            "2021-09-11 17:48:46,693 influence features loaded!\n",
            "2021-09-11 17:48:46,695 labels loaded!\n",
            "2021-09-11 17:48:46,701 vertex ids loaded!\n",
            "2021-09-11 17:48:46,820 global vertex features loaded!\n",
            "2021-09-11 17:48:56,878 64-dim embedding loaded!\n",
            "2021-09-11 17:48:57,324 24428 ego networks loaded, each with size 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGPkIH-mRBzZ",
        "outputId": "f49dd038-9372-42d2-d5a2-ce2c50910a98"
      },
      "source": [
        "N = len(influence_dataset)\n",
        "n_classes = 2\n",
        "class_weight = influence_dataset.get_class_weight() \\\n",
        "        if args.class_weight_balanced else torch.ones(n_classes)\n",
        "logger.info(\"class_weight=%.2f:%.2f\", class_weight[0], class_weight[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 17:49:00,656 class_weight=1.00:1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OZ3yW9HSx71",
        "outputId": "8602a712-1a0b-41b0-8429-2da95f5505af"
      },
      "source": [
        "feature_dim = influence_dataset.get_feature_dimension()\n",
        "n_units = [feature_dim] + [int(x) for x in args.hidden_units.strip().split(\",\")] + [n_classes]\n",
        "logger.info(\"feature dimension=%d\", feature_dim)\n",
        "logger.info(\"number of classes=%d\", n_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 17:49:03,833 feature dimension=2\n",
            "2021-09-11 17:49:03,836 number of classes=2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8gckmx0iSNd"
      },
      "source": [
        "train_start,  valid_start, test_start = \\\n",
        "        0, int(N * args.train_ratio / 100), int(N * (args.train_ratio + args.valid_ratio) / 100)\n",
        "train_loader = DataLoader(influence_dataset, batch_size=args.batch,\n",
        "                        sampler=ChunkSampler(valid_start - train_start, 0))\n",
        "valid_loader = DataLoader(influence_dataset, batch_size=args.batch,\n",
        "                        sampler=ChunkSampler(test_start - valid_start, valid_start))\n",
        "test_loader = DataLoader(influence_dataset, batch_size=args.batch,\n",
        "                        sampler=ChunkSampler(N - test_start, test_start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-iDMv6sqtkX"
      },
      "source": [
        "class BatchGraphConvolution(Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(BatchGraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "            init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, x, lap):\n",
        "        expand_weight = self.weight.expand(x.shape[0], -1, -1)\n",
        "        support = torch.bmm(x, expand_weight)\n",
        "        output = torch.bmm(lap, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7YADToFqmFG"
      },
      "source": [
        "class BatchGCN(nn.Module):\n",
        "    def __init__(self, n_units, dropout, pretrained_emb, vertex_feature,\n",
        "            use_vertex_feature, fine_tune=False, instance_normalization=False):\n",
        "        super(BatchGCN, self).__init__()\n",
        "        self.num_layer = len(n_units) - 1\n",
        "        self.dropout = dropout\n",
        "        self.inst_norm = instance_normalization\n",
        "        if self.inst_norm:\n",
        "            self.norm = nn.InstanceNorm1d(pretrained_emb.size(1), momentum=0.0, affine=True)\n",
        "\n",
        "        # https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222/2\n",
        "        self.embedding = nn.Embedding(pretrained_emb.size(0), pretrained_emb.size(1))\n",
        "        self.embedding.weight = nn.Parameter(pretrained_emb)\n",
        "        self.embedding.weight.requires_grad = fine_tune\n",
        "        n_units[0] += pretrained_emb.size(1)\n",
        "\n",
        "        self.use_vertex_feature = use_vertex_feature\n",
        "        if self.use_vertex_feature:\n",
        "            self.vertex_feature = nn.Embedding(vertex_feature.size(0), vertex_feature.size(1))\n",
        "            self.vertex_feature.weight = nn.Parameter(vertex_feature)\n",
        "            self.vertex_feature.weight.requires_grad = False\n",
        "            n_units[0] += vertex_feature.size(1)\n",
        "\n",
        "        self.layer_stack = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.num_layer):\n",
        "            self.layer_stack.append(\n",
        "                    BatchGraphConvolution(n_units[i], n_units[i + 1])\n",
        "                    )\n",
        "\n",
        "    def forward(self, x, vertices, lap):\n",
        "        emb = self.embedding(vertices)\n",
        "        if self.inst_norm:\n",
        "            emb = self.norm(emb.transpose(1, 2)).transpose(1, 2)\n",
        "        x = torch.cat((x, emb), dim=2)\n",
        "        if self.use_vertex_feature:\n",
        "            vfeature = self.vertex_feature(vertices)\n",
        "            x = torch.cat((x, vfeature), dim=2)\n",
        "        for i, gcn_layer in enumerate(self.layer_stack):\n",
        "            x = gcn_layer(x, lap)\n",
        "            if i + 1 < self.num_layer:\n",
        "                x = F.elu(x)\n",
        "                x = F.dropout(x, self.dropout, training=self.training)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teNCj-6zjqCq"
      },
      "source": [
        "class MultiHeadGraphAttention(nn.Module):\n",
        "    def __init__(self, n_head, f_in, f_out, attn_dropout, bias=True):\n",
        "        super(MultiHeadGraphAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.w = Parameter(torch.Tensor(n_head, f_in, f_out))\n",
        "        self.a_src = Parameter(torch.Tensor(n_head, f_out, 1))\n",
        "        self.a_dst = Parameter(torch.Tensor(n_head, f_out, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(f_out))\n",
        "            init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        init.xavier_uniform_(self.w)\n",
        "        init.xavier_uniform_(self.a_src)\n",
        "        init.xavier_uniform_(self.a_dst)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        n = h.size(0) # h is of size n x f_in\n",
        "        h_prime = torch.matmul(h.unsqueeze(0), self.w) #  n_head x n x f_out\n",
        "        attn_src = torch.bmm(h_prime, self.a_src) # n_head x n x 1\n",
        "        attn_dst = torch.bmm(h_prime, self.a_dst) # n_head x n x 1\n",
        "        attn = attn_src.expand(-1, -1, n) + attn_dst.expand(-1, -1, n).permute(0, 2, 1) # n_head x n x n\n",
        "\n",
        "        attn = self.leaky_relu(attn)\n",
        "        attn.data.masked_fill_(1 - adj, float(\"-inf\"))\n",
        "        attn = self.softmax(attn) # n_head x n x n\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.bmm(attn, h_prime) # n_head x n x f_out\n",
        "\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "class BatchMultiHeadGraphAttention(nn.Module):\n",
        "    def __init__(self, n_head, f_in, f_out, attn_dropout, bias=True):\n",
        "        super(BatchMultiHeadGraphAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.w = Parameter(torch.Tensor(n_head, f_in, f_out))\n",
        "        self.a_src = Parameter(torch.Tensor(n_head, f_out, 1))\n",
        "        self.a_dst = Parameter(torch.Tensor(n_head, f_out, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(f_out))\n",
        "            init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        init.xavier_uniform_(self.w)\n",
        "        init.xavier_uniform_(self.a_src)\n",
        "        init.xavier_uniform_(self.a_dst)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        bs, n = h.size()[:2] # h is of size bs x n x f_in\n",
        "        h_prime = torch.matmul(h.unsqueeze(1), self.w) # bs x n_head x n x f_out\n",
        "        attn_src = torch.matmul(torch.tanh(h_prime), self.a_src) # bs x n_head x n x 1\n",
        "        attn_dst = torch.matmul(torch.tanh(h_prime), self.a_dst) # bs x n_head x n x 1\n",
        "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(0, 1, 3, 2) # bs x n_head x n x n\n",
        "\n",
        "        attn = self.leaky_relu(attn)\n",
        "        mask = 1 - adj # bs x 1 x n x n\n",
        "        attn.data.masked_fill_(mask, float(\"-inf\"))\n",
        "        attn = self.softmax(attn) # bs x n_head x n x n\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, h_prime) # bs x n_head x n x f_out\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd4XV8IdjzUK"
      },
      "source": [
        "class BatchGAT(nn.Module):\n",
        "    def __init__(self, pretrained_emb, vertex_feature, use_vertex_feature,\n",
        "            n_units=[1433, 8, 7], n_heads=[8, 1],\n",
        "            dropout=0.1, attn_dropout=0.0, fine_tune=False,\n",
        "            instance_normalization=False):\n",
        "        super(BatchGAT, self).__init__()\n",
        "        self.n_layer = len(n_units) - 1\n",
        "        self.dropout = dropout\n",
        "        self.inst_norm = instance_normalization\n",
        "        if self.inst_norm:\n",
        "            self.norm = nn.InstanceNorm1d(pretrained_emb.size(1), momentum=0.0, affine=True)\n",
        "\n",
        "        # https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222/2\n",
        "        self.embedding = nn.Embedding(pretrained_emb.size(0), pretrained_emb.size(1))\n",
        "        self.embedding.weight = nn.Parameter(pretrained_emb)\n",
        "        self.embedding.weight.requires_grad = fine_tune\n",
        "        n_units[0] += pretrained_emb.size(1)\n",
        "\n",
        "        self.use_vertex_feature = use_vertex_feature\n",
        "        if self.use_vertex_feature:\n",
        "            self.vertex_feature = nn.Embedding(vertex_feature.size(0), vertex_feature.size(1))\n",
        "            self.vertex_feature.weight = nn.Parameter(vertex_feature)\n",
        "            self.vertex_feature.weight.requires_grad = False\n",
        "            n_units[0] += vertex_feature.size(1)\n",
        "\n",
        "        self.layer_stack = nn.ModuleList()\n",
        "        for i in range(self.n_layer):\n",
        "            # consider multi head from last layer\n",
        "            f_in = n_units[i] * n_heads[i - 1] if i else n_units[i]\n",
        "            self.layer_stack.append(\n",
        "                    BatchMultiHeadGraphAttention(n_heads[i], f_in=f_in,\n",
        "                        f_out=n_units[i + 1], attn_dropout=attn_dropout)\n",
        "                    )\n",
        "\n",
        "    def forward(self, x, vertices, adj):\n",
        "        emb = self.embedding(vertices)\n",
        "        if self.inst_norm:\n",
        "            emb = self.norm(emb.transpose(1, 2)).transpose(1, 2)\n",
        "        x = torch.cat((x, emb), dim=2)\n",
        "        if self.use_vertex_feature:\n",
        "            vfeature = self.vertex_feature(vertices)\n",
        "            x = torch.cat((x, vfeature), dim=2)\n",
        "        bs, n = adj.size()[:2]\n",
        "        for i, gat_layer in enumerate(self.layer_stack):\n",
        "            x = gat_layer(x, adj) # bs x n_head x n x f_out\n",
        "            if i + 1 == self.n_layer:\n",
        "                x = x.mean(dim=1)\n",
        "            else:\n",
        "                x = F.elu(x.transpose(1, 2).contiguous().view(bs, n, -1))\n",
        "                x = F.dropout(x, self.dropout, training=self.training)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRobHX1FkBnI"
      },
      "source": [
        "model = BatchGCN(pretrained_emb=influence_dataset.get_embedding(),\n",
        "            vertex_feature=influence_dataset.get_vertex_features(),\n",
        "            use_vertex_feature=args.use_vertex_feature,\n",
        "            n_units=n_units,\n",
        "            dropout=args.dropout,\n",
        "            instance_normalization=args.instance_normalization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzzIhzakkUV2"
      },
      "source": [
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    class_weight = class_weight.cuda()\n",
        "\n",
        "params = [{'params': filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if args.model == \"pscn\" else model.layer_stack.parameters()}]\n",
        "\n",
        "optimizer = optim.Adagrad(params, lr=args.lr, weight_decay=args.weight_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEa6EgGQkicu"
      },
      "source": [
        "def evaluate(epoch, loader, thr=None, return_best_thr=False, log_desc='valid_'):\n",
        "    model.eval()\n",
        "    total = 0.\n",
        "    loss, prec, rec, f1 = 0., 0., 0., 0.\n",
        "    y_true, y_pred, y_score = [], [], []\n",
        "    for i_batch, batch in enumerate(loader):\n",
        "        graph, features, labels, vertices = batch\n",
        "        bs = graph.size(0)\n",
        "\n",
        "        if args.cuda:\n",
        "            features = features.cuda()\n",
        "            graph = graph.cuda()\n",
        "            labels = labels.cuda()\n",
        "            vertices = vertices.cuda()\n",
        "\n",
        "        output = model(features, vertices, graph)\n",
        "        if args.model == \"gcn\" or args.model == \"gat\":\n",
        "            output = output[:, -1, :]\n",
        "        loss_batch = F.nll_loss(output, labels, class_weight)\n",
        "        loss += bs * loss_batch.item()\n",
        "\n",
        "        y_true += labels.data.tolist()\n",
        "        y_pred += output.max(1)[1].data.tolist()\n",
        "        y_score += output[:, 1].data.tolist()\n",
        "        total += bs\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    if thr is not None:\n",
        "        logger.info(\"using threshold %.4f\", thr)\n",
        "        y_score = np.array(y_score)\n",
        "        y_pred = np.zeros_like(y_score)\n",
        "        y_pred[y_score > thr] = 1\n",
        "\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
        "    auc = roc_auc_score(y_true, y_score)\n",
        "    logger.info(\"%sloss: %.4f AUC: %.4f Prec: %.4f Rec: %.4f F1: %.4f\",\n",
        "            log_desc, loss / total, auc, prec, rec, f1)\n",
        "\n",
        "    tensorboard_logger.Logger(log_desc + 'loss', loss / total, epoch + 1)\n",
        "    tensorboard_logger.Logger(log_desc + 'auc', auc, epoch + 1)\n",
        "    tensorboard_logger.Logger(log_desc + 'prec', prec, epoch + 1)\n",
        "    tensorboard_logger.Logger(log_desc + 'rec', rec, epoch + 1)\n",
        "    tensorboard_logger.Logger(log_desc + 'f1', f1, epoch + 1)\n",
        "\n",
        "    if return_best_thr:\n",
        "        precs, recs, thrs = precision_recall_curve(y_true, y_score)\n",
        "        f1s = 2 * precs * recs / (precs + recs)\n",
        "        f1s = f1s[:-1]\n",
        "        thrs = thrs[~np.isnan(f1s)]\n",
        "        f1s = f1s[~np.isnan(f1s)]\n",
        "        best_thr = thrs[np.argmax(f1s)]\n",
        "        logger.info(\"best threshold=%4f, f1=%.4f\", best_thr, np.max(f1s))\n",
        "        return best_thr\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FocD7AgWkm_G"
      },
      "source": [
        "def train(epoch, train_loader, valid_loader, test_loader, log_desc='train_'):\n",
        "    model.train()\n",
        "\n",
        "    loss = 0.\n",
        "    total = 0.\n",
        "    for i_batch, batch in enumerate(train_loader):\n",
        "        graph, features, labels, vertices = batch\n",
        "        bs = graph.size(0)\n",
        "\n",
        "        if args.cuda:\n",
        "            features = features.cuda()\n",
        "            graph = graph.cuda()\n",
        "            labels = labels.cuda()\n",
        "            vertices = vertices.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features, vertices, graph)\n",
        "        if args.model == \"gcn\" or args.model == \"gat\":\n",
        "            output = output[:, -1, :]\n",
        "        loss_train = F.nll_loss(output, labels, class_weight)\n",
        "        loss += bs * loss_train.item()\n",
        "        total += bs\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "    logger.info(\"train loss in this epoch %f\", loss / total)\n",
        "    tensorboard_logger.Logger('train_loss', loss / total, epoch + 1)\n",
        "    if (epoch + 1) % args.check_point == 0:\n",
        "        logger.info(\"epoch %d, checkpoint!\", epoch)\n",
        "        best_thr = evaluate(epoch, valid_loader, return_best_thr=True, log_desc='valid_')\n",
        "        evaluate(epoch, test_loader, thr=best_thr, log_desc='test_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3hIXW9Okst2"
      },
      "source": [
        "# Train model\n",
        "\n",
        "t_total = time.time()\n",
        "logger.info(\"training...\")\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch, train_loader, valid_loader, test_loader)\n",
        "logger.info(\"optimization Finished!\")\n",
        "logger.info(\"total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "logger.info(\"retrieve best threshold...\")\n",
        "best_thr = evaluate(args.epochs, valid_loader, return_best_thr=True, log_desc='valid_')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}